set tcm3p = $op/tests/tcm3


***************************************************************************
****** Sat July 9, 2022

TEST HYPRE

**SET ORDER FOR TCM3 !!

mpirun -np 1 $tcm3p -g=square1024.order4 -order=4 -solver=petsc -predefined -dirichlet -debug=1 -hypre -tol=1e-12


***************************************************************************
****** Fri July 8, 2022

TEST HYPRE


++++ SHAPES

mpirun -np 1 $tcm3p -g=shapesBige16.order4.ml3.hdf -solver=petsc -predefined -dirichlet -debug=1 -hypre -tol=1e-10

PETScSolver::Time for solve=7.94e+00 (its=22) 

*** max residual=1.85e-07, time for 1st solve of the Dirichlet problem = 1.08e+01 (iterations=22) ***
PETScSolver::Time for solve=6.32e+00 (its=22) 

*** max residual=1.85e-07, time for 2nd solve of the Dirichlet problem = 6.41e+00 (iterations=22) ***

.....solver: size = 2.09e+08 (bytes), grid-pts=1060956, reals/grid-pt=24.63 



++++++++++++ CIC +++++++
mpirun -np 1 $tcm3p -g=cice32.order4 -solver=petsc -predefined -dirichlet -debug=1 -hypre -tol=1e-10


PETScSolver::Time for solve=1.21e+01 (its=20) 

*** max residual=9.34e-08, time for 1st solve of the Dirichlet problem = 1.68e+01 (iterations=20) ***
PETScSolver::Time for solve=9.43e+00 (its=20) 

*** max residual=9.34e-08, time for 2nd solve of the Dirichlet problem = 9.58e+00 (iterations=20) ***

.....solver: size = 3.30e+08 (bytes), grid-pts=1666855, reals/grid-pt=24.73 
 grid=0 (square) max. rel. err=8.262364e-10 (8.262364e-10 with ghost)
 grid=1 (Annulus) max. rel. err=5.300603e-09 (5.305551e-09 with ghost)
Maximum relative error with dirichlet bc's= 5.300603e-09 (5.305551e-09 with ghost)




++++++ SQUARE:

decrease tol:
mpirun -np 1 $tcm3p -g=square1024.order4 -solver=petsc -predefined -dirichlet -debug=1 -hypre -tol=1e-12

PETScSolver::Time for solve=4.02e+00 (its=8) 

*** max residual=1.00e-09, time for 1st solve of the Dirichlet problem = 7.14e+00 (iterations=8) ***
PETScSolver::Time for solve=2.49e+00 (its=8) 

*** max residual=1.00e-09, time for 2nd solve of the Dirichlet problem = 2.59e+00 (iterations=8) ***



mpirun -np 1 $tcm3p -g=square1024.order4 -solver=petsc -predefined -dirichlet -debug=1 -hypre 


 === Solver:
 PETScNew, parallel=[generalized minimal residual iteration + hypre], local=[+hypre + reverse Cuthill-McKee ordering]
 =====
PETScSolver:: build matrix... Oges::debug=1
PETScSolver::NEW: grid=0 p=0 local bounds=[-2,1026][-2,1026][0,0] 
--PES-- Time for counting equations= 1.40e+00(s)
--PES--- Time for assembling matrices= 2.52e-02(s)
PETScSolver:: ... done build matrix, cpu=3.02e+00
PETScSolver::buildSolver: INFO: adding option=[-ksp_type] value=[gmres]
PETScSolver::buildSolver: INFO: adding option=[-pc_type] value=[hypre]
PETScSolver::buildSolver: INFO: adding option=[-pc_hypre_type] value=[boomeramg]
PETScSolver::buildSolver: INFO: adding option=[-pc_hypre_boomeramg_strong_threshold] value=[.5]
PETScSolver::buildSolver: INFO: adding option=[-pc_hypre_boomeramg_max_levels] value=[20]
PETScSolver::buildSolver: INFO: adding option=[-pc_hypre_boomeramg_coarsen_type] value=[Falgout]
*** -pc_type found in the options database: [hypre]

BoomerAMG SETUP PARAMETERS:

 Max levels = 20
 Num levels = 14

 Strength Threshold = 0.500000
 Interpolation Truncation Factor = 0.000000
 Maximum Row Sum Threshold for Dependency Weakening = 0.900000

 Coarsening Type = Falgout-CLJP 
 measures are determined locally

 Interpolation = modified classical interpolation

Operator Matrix Information:

            nonzero         entries per row        row sums
lev   rows  entries  sparse  min  max   avg       min         max
===================================================================
 0 1058841  5257269  0.000     1    5   5.0   0.000e+00   1.000e+00
 1  523265  4701201  0.000     4    9   9.0  -3.125e-01   0.000e+00
 2  261121  5461077  0.000     8   21  20.9  -1.919e-01   0.000e+00
 3  130561  3243601  0.000     9   25  24.8  -2.337e-01   0.000e+00
 4   65025  2379465  0.001    13   37  36.6  -1.631e-01   0.000e+00
 5   32513  1437665  0.001    14   45  44.2  -1.939e-01   0.000e+00
 6    8194   199272  0.003     7   25  24.3  -2.262e-01   0.000e+00
 7    2050    48564  0.012     7   25  23.7  -2.436e-01   0.000e+00
 8     557    12453  0.040     6   27  22.4  -2.825e-01   3.816e-17
 9     182     3744  0.113     8   29  20.6  -2.826e-01   1.124e-16
10      70     1206  0.246     8   27  17.2  -2.583e-01   0.000e+00
11      27      411  0.564    10   27  15.2  -2.741e-01   0.000e+00
12      10       96  0.960     9   10   9.6  -3.284e-01   0.000e+00
13       4       16  1.000     4    4   4.0  -2.912e-01   0.000e+00


Interpolation Matrix Information:
                 entries/row    min     max         row sums
lev  rows cols    min max     weight   weight     min       max 
=================================================================
 0 1058841 x 523265   0   4   2.500e-01 2.500e-01 0.000e+00 1.000e+00
 1 523265 x 261121   1   4   1.667e-01 2.500e-01 1.667e-01 1.000e+00
 2 261121 x 130561   1   4   1.983e-01 2.500e-01 6.007e-01 1.000e+00
 3 130561 x 65025   1   4   1.407e-01 2.500e-01 1.407e-01 1.000e+00
 4 65025 x 32513   1   4   1.625e-01 2.500e-01 4.989e-01 1.000e+00
 5 32513 x 8194    1   4   1.235e-01 5.000e-01 1.235e-01 1.000e+00
 6  8194 x 2050    1   4   7.669e-02 5.150e-01 7.669e-02 1.000e+00
 7  2050 x 557     0   5   4.280e-02 5.000e-01 0.000e+00 1.000e+00
 8   557 x 182     0   5   4.185e-02 5.751e-01 0.000e+00 1.000e+00
 9   182 x 70      1   5   5.328e-02 7.126e-01 9.587e-02 1.000e+00
10    70 x 27      1   5   6.136e-02 6.450e-01 1.551e-01 1.000e+00
11    27 x 10      0   4   8.138e-02 2.811e-01 0.000e+00 1.000e+00
12    10 x 4       0   3   4.006e-02 1.681e-01 0.000e+00 1.000e+00


     Complexity:    grid = 1.966698
                operator = 4.326589




BoomerAMG SOLVER PARAMETERS:

  Maximum number of cycles:         1 
  Stopping Tolerance:               0.000000e+00 
  Cycle type (1 = V, 2 = W, etc.):  1

  Relaxation Parameters:
   Visiting Grid:                     down   up  coarse
            Number of sweeps:            1    1     1 
   Type 0=Jac, 3=hGS, 6=hSGS, 9=GE:      6    6     9 
   Point types, partial sweeps (1=C, -1=F):
                  Pre-CG relaxation (down):   1  -1
                   Post-CG relaxation (up):  -1   1
                             Coarsest grid:   0

  0 KSP Residual norm 2.468301119096e+03 
  1 KSP Residual norm 9.164929478948e+01 
  2 KSP Residual norm 4.008777702260e+00 
  3 KSP Residual norm 9.670755069880e-02 
  4 KSP Residual norm 2.455477655574e-03 
  5 KSP Residual norm 5.842864580656e-05 
  6 KSP Residual norm 1.424968092018e-06 
PETScSolver::Time for solve=3.53e+00 (its=6) 

*** max residual=1.42e-06, time for 1st solve of the Dirichlet problem = 6.65e+00 (iterations=6) ***
  0 KSP Residual norm 2.468301119096e+03 
  1 KSP Residual norm 9.164929478948e+01 
  2 KSP Residual norm 4.008777702260e+00 
  3 KSP Residual norm 9.670755069880e-02 
  4 KSP Residual norm 2.455477655574e-03 
  5 KSP Residual norm 5.842864580656e-05 
  6 KSP Residual norm 1.424968092018e-06 
PETScSolver::Time for solve=1.98e+00 (its=6) 

*** max residual=1.42e-06, time for 2nd solve of the Dirichlet problem = 2.08e+00 (iterations=6) ***

.....solver: size = 2.11e+08 (bytes), grid-pts=1058841, reals/grid-pt=24.95 
 grid=0 (square) max. rel. err=2.874330e-09 (3.370111e-09 with ghost)
Maximum relative error with dirichlet bc's= 2.874330e-09 (3.370111e-09 with ghost)
petsc/square1024.order4///dirichlet: error                                                  : err = 2.87e-09, cpu=2.1e+00(s)  






*** max residual=1.42e-06, time for 1st solve of the Dirichlet problem = 6.62e+00 (iterations=6) ***
PETScSolver::Time for solve=4.83e-01 (its=0) 


Maybe works: 
mpirun -np 1 $tcm3p -g=square32.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

***************************************************************************
****** Fri June 10, 2022

make testExtrapInterpNeighbours

testExtrapInterpNeighbours -g=sise2.order2 -useNew=1 -interpolateNeighbours=1 -debug=3

testExtrapInterpNeighbours -g=cice2.order2 -useNew=1 -interpolateNeighbours=1 -debug=3

interpNeighbours: 196 pts interpolated max error =2.01e-05 
cice2.order2/square///extrapInterpNeighbours                                                : err = 2.01e-05, cpu=0.0e+00(s)  
cice2.order2/Annulus///extrapInterpNeighbours                                               : err = 6.66e-16, cpu=0.0e+00(s)  


***************************************************************************
****** Sun/Mon Jun 5/6, 2022

Revisit extrapolate interp neighbours 
Add option to interpolate interp neighbours

make testExtrapInterpNeighbours

testExtrapInterpNeighbours -g=sise2.order2 -useNew=1 -interpolateNeighbours=1 -debug=3

testExtrapInterpNeighbours -g=cice2.order2 -useNew=1 -interpolateNeighbours=1 -debug=3


testExtrapInterpNeighbours -g=cice2.order2 -useNew=1 -debug=3

testExtrapInterpNeighbours -g=cice2.order4 -useNew=1 -debug=3  **trouble***
testExtrapInterpNeighbours -g=cice2.order4.ng3 -useNew=1 -debug=3   ** OK **

testExtrapInterpNeighbours -g=sise2.order2 -useNew=1
testExtrapInterpNeighbours -g=sise2.order4 -useNew=1 -debug=3

testExtrapInterpNeighbours -g=bibe2.order2 -useNew=1

***************************************************************************
****** June 21, 201

set tcm3p = $op/tests/tcm3

OK: 
mpirun -np 8 $tcm3p -g=sphereInABoxe2.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

-np=16: 
PETScSolver:: ... done build matrix, cpu=8.80e+00
*** max residual=4.91e-05, time for 1st solve of the Dirichlet problem = 1.34e+01 (iterations=18) ***


-np=8:

  0 KSP Residual norm 1.469593874811e+04 
  1 KSP Residual norm 1.472487215815e+03 
  2 KSP Residual norm 3.348992446759e+02 
  3 KSP Residual norm 1.166775517819e+02 
  4 KSP Residual norm 6.481240170094e+01 
  5 KSP Residual norm 1.626359096344e+01 
  6 KSP Residual norm 7.285976334596e+00 
  7 KSP Residual norm 3.028336681741e+00 
  8 KSP Residual norm 6.033370037976e-01 
  9 KSP Residual norm 2.276687845151e-01 
 10 KSP Residual norm 8.366548331167e-02 
 11 KSP Residual norm 2.665167857718e-02 
 12 KSP Residual norm 1.035101935620e-02 
 13 KSP Residual norm 3.401610162266e-03 
 14 KSP Residual norm 1.275335077621e-03 
 15 KSP Residual norm 4.364253507439e-04 
 16 KSP Residual norm 1.441542358314e-04 
PETScSolver::Time for solve=6.52e+00 (its=16) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]

*** max residual=1.44e-04, time for 1st solve of the Dirichlet problem = 2.83e+01 (iterations=16) ***



OK: *trouble -np=2** ??
$tcm3p -g=sphereInABoxe2.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre
*** -pc_type found in the options database: [hypre]
  0 KSP Residual norm 1.504580129598e+04 
  1 KSP Residual norm 1.042437246207e+03 
  2 KSP Residual norm 1.247567022542e+02 
  3 KSP Residual norm 3.843323126131e+01 
  4 KSP Residual norm 1.284686522264e+01 
  5 KSP Residual norm 7.287464088270e+00 
  6 KSP Residual norm 3.058038323502e+00 
  7 KSP Residual norm 6.211494531145e-01 
  8 KSP Residual norm 3.124334411907e-01 
  9 KSP Residual norm 9.721372129836e-02 
 10 KSP Residual norm 4.033251740458e-02 
 11 KSP Residual norm 1.769314060239e-02 
 12 KSP Residual norm 7.324545544134e-03 
 13 KSP Residual norm 3.893201613319e-03 
 14 KSP Residual norm 9.017060151107e-04 
 15 KSP Residual norm 2.288617256085e-04 
 16 KSP Residual norm 6.814668086022e-05 
PETScSolver::Time for solve=1.71e+01 (its=16) 

OK: 
mpirun -np 4 $tcm3p -g=sphereInABoxe1.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

OK:
mpirun -np 8 $tcm3p -g=cice16.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

OK: 
$tcm3p -g=cice4.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

*** -pc_type found in the options database: [hypre]
  0 KSP Residual norm 6.547554155602e+02 
  1 KSP Residual norm 5.158686195998e+01 
  2 KSP Residual norm 4.478637225667e+00 
  3 KSP Residual norm 6.937503037368e-01 
  4 KSP Residual norm 1.363014275258e-01 
  5 KSP Residual norm 2.280934521198e-02 
  6 KSP Residual norm 3.064811032451e-03 
  7 KSP Residual norm 6.889056098088e-04 
  8 KSP Residual norm 8.505328891671e-05 
  9 KSP Residual norm 2.562108182655e-05 
 10 KSP Residual norm 3.095166775547e-06 
PETScSolver::Time for solve=1.42e-01 (its=10) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]



TEST HYPRE: works
$tcm3p -g=square64.order2 -solver=petsc -predefined -dirichlet -debug=1 -hypre

  0 KSP Residual norm 1.761875507209e+02 
  1 KSP Residual norm 8.877005411439e+00 
  2 KSP Residual norm 1.937521081133e-01 
  3 KSP Residual norm 3.019956446744e-03 
  4 KSP Residual norm 5.431298376278e-05 
  5 KSP Residual norm 6.613549892860e-07 
PETScSolver::Time for solve=1.55e-02 (its=5) 
--PETSc-- solver: ksp[gmres] pc[hypre-boomeramg]

*** max residual=6.61e-07, time for 1st solve of the Dirichlet problem = 2.84e-02 (iterations=5) ***


tcm3p -g=square20.order2 -solver=petsc -predefined


set tcmcp = $op/tests/tcmConstraintp
set tcmcs = $op/tests/tcmConstraints

TEST BOX:

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=
BOX4
>>>>>>>>>  -np=1
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PS-- fill in extra eqn =0
--PS-- time to insert 1682 values for eqn 0 is  1.43e+00
--PS-- fill in extra eqn =1
--PS-- time to insert 1682 values for eqn 1 is  1.32e+00
--PES-- Time for filling user extra equations= 2.75e+00(s)
PETScSolver:: ... done build matrix, cpu=3.01e+00
PETScSolver::Time for solve=2.06e-01 (its=33) 

>>>>>>>> -np=2
-PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PS-- fill in extra eqn =0
--PS-- myid=0 time to insert 821 values for eqn 0 is  2.81e-05
--PS-- myid=1 time to insert 862 values for eqn 0 is  2.49e-01
--PS-- fill in extra eqn =1
--PS-- myid=0 time to insert 821 values for eqn 1 is  1.91e-05
--PS-- myid=1 time to insert 862 values for eqn 1 is  2.39e-01
--PES-- Time for filling user extra equations= 4.89e-01(s)
PETScSolver:: ... done build matrix, cpu=6.45e-01


>>>>>> -np=4
--PS-- fill in extra eqn =0
--PS-- myid=1 time to insert 1 values for eqn 0 is  1.26e-05
--PS-- myid=3 time to insert 1 values for eqn 0 is  9.54e-07
--PS-- myid=0 time to insert 821 values for eqn 0 is  1.98e-05
--PS-- myid=2 time to insert 862 values for eqn 0 is  1.81e-05
--PS-- fill in extra eqn =1
--PS-- myid=1 time to insert 1 values for eqn 1 is  4.77e-07
--PS-- myid=3 time to insert 1 values for eqn 1 is  4.77e-07
--PS-- myid=0 time to insert 821 values for eqn 1 is  1.03e-05
--PS-- myid=2 time to insert 862 values for eqn 1 is  6.91e-06
--PES-- Time for filling user extra equations= 7.93e-03(s)
PETScSolver:: ... done build matrix, cpu=9.06e-02





++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++=


SERIAL
$tcmcs box4.order2.hdf -solver=petsc -predefined -debug=1 
+Petsc TIMINGS (for 33 its, size of matrix n = 91125 ):
 build=0.06125, precond=0.063491, solve=0.207083, total=0.331824.

Neumann: 
+Petsc TIMINGS (for 56 its, size of matrix n = 91125 ):
 build=0.098647, precond=0.948522, solve=0.2984, total=1.34557.

PARALLEL: 
mpirun -np 1 $tcmcp box4.order2.hdf -solver=petsc -predefined -debug=1 
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PES-- Time for filling user extra equations= 2.64e+00(s)
PETScSolver:: ... done build matrix, cpu=2.89e+00
PETScSolver::Time for solve=2.06e-01 (its=33) 

Neumann ***
PETScSolver:: build matrix... Oges::debug=1
--PS-- start and end equation numbers for each grid:
     grid=0 [eqnStart,eqnEnd]=[0,91124]
--PES-- Time for filling user extra equations= 5.32e-05(s)
PETScSolver:: ... done build matrix, cpu=5.74e+01
PETScSolver::Time for solve=1.24e+00 (its=54) 



mpirun -np 1 $tcmcp box2.order2.hdf -solver=petsc -predefined -debug=1 

mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined -debug=1


***************************************************************************
****** June 3,4 2017

*** CHANGED PETSC-SOLVER TO USE OGES EQUATION NUMBERING TOO


*NEW*
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np4.out


OLD:
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out


*** CHANGE PETScSOLVER to put extra equations at the end of the matrix 


mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tc.np1.out




set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined -trig

Maximum relative error with dirichlet bc's= 8.996811e-03 (8.996811e-03 with ghost)
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value= 1.958 true= 2.000 err=4.24e-02
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value= 0.938 true= 1.000 err=6.25e-02

Maximum relative error with neumann bc's= 8.375221e-03
--TCMC-- i=0 extra=1 extraEquationNumber(0) : constraint value=11.005 true=11.000 err=4.51e-03
--TCMC-- i=1 extra=0 extraEquationNumber(1) : constraint value=-0.000 true= 0.000 err=6.54e-13



***************************************************************************
****** June 2, 2017


TEST TCMCONSTRAINT ON overset grid


**** SLIDER GRID IS PERIODIC IN X ***

set tcmcp = $op/tests/tcmConstraintp 
mpirun -np 1 $tcmcp sliderRefineGride2.order2 -solver=petsc -predefined 

SERIAL: NOT EXACT **WHY**
$tcmcs sliderRefineGride2.order2 -solver=petsc -predefined  > sr.serial.out

WORKS NOW: 
mpirun -np 1 $tcmcp plug2.hdf -solver=petsc -predefined > plug.np1.out
mpirun -np 1 $tcmcp plug1.hdf -solver=petsc -predefined > plug1.np1.out
SERIAL PLUG -- exact 
$tcmcs plug2.hdf -solver=petsc -predefined  > plug.serial.out
$tcmcs plug1.hdf -solver=petsc -predefined  > plug1.serial.out

mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out



***************************************************************************
****** June 1, 2017

**CHANGES TO USER SUPPLIED EQUATIONS IN PARALLEL***

*NEW* TEST:

  
WORKS: 
mpirun -np 3 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined
mpirun -np 2 valgrind --log-file=tc.out.np%p $tcmcp square8.order2.hdf -solver=petsc -predefined

mpirun -np 1 valgrind $tcmcp square8.order2.hdf -solver=petsc -predefined

-- looks OK, check valgrind
mpirun -np 1 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np1.out
mpirun -np 2 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np2.out
mpirun -np 3 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np3.out
mpirun -np 4 $tcmcp square8.order2.hdf -solver=petsc -predefined > tcmc.np4.out



set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined
$tcmcs square8.order2.hdf -solver=petsc -predefined




***************************************************************************
****** May 23, 2017

ANOTHER BUG -- Shift equationNumber base in PETScSolver to eqnBase=1

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelpNew.out

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=15 > ! matParallelNew.out



***************************************************************************
****** May 21-22, 2017

** ANOTHER BUG FOUND with predefined and user-defined equations

**BUG FOUND WITH PARALLEL and PREDEFINED -- constraint equation bad


CHECK tcmConstraint + predefined:

set tcmcp = $op/tests/tcmConstraintp 
set tcmcs = $op/tests/tcmConstraints

$tcmcp square8.order2.hdf -solver=petsc -predefined > ! tcmcParallelp.out
$tcmcs square8.order2.hdf -solver=petsc -predefined > ! tcmcSerials.out



! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matParallelp.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -predefined > ! matSerialp.out

! These match now:
tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matParallel.out
tcm3s -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -debug=7 > ! matSerial.out

***************************************************************************
****** May 21, 2017


Tracking down bug with parallel AMP:

alias tcm3p $op/tests/tcm3p
alias tcm3s $op/tests/tcm3s

tcm3p -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matParallel.out

tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc > ! matSerial.out


tcm3 -g=shearBlockCoarseGrid2.order2.hdf -solver=petsc -neumann -outputMatrix


***************************************************************************
****** April 1-2, 2017


MORE WORK ON CONSTRAINTS IN PARALLEL PETSC

PRINT OUT SPARSE MATRIX

mpiexec -n 2 xterm -e gdb --args $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12

PARALLEL
mpirun -np 1 $tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.parallel.out


SERIAL
$tcmc square5.order2.hdf -solver=petsc -tol=1.e-12 >! tcmc.serial.out


***************************************************************************
****** March 13-20, 2017

Implement constraints in parallel petsc

mpiexec -n 2 xterm -e gdb --args $tcmc square8.order2.hdf -solver=petsc


set tcmc = $op/tests/tcmConstraint

$tcmc square8.order2.hdf -solver=petsc

mpirun -np 1 $tcmc square8.order2.hdf -solver=petsc



  


============================
==== Aug 27, 2016

----- TEST NEW opt version of 6th-order Laplacian ----

mpiexec -n 4 tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc -tol=1.e-12
 grid=0 (square) max. rel. err=1.599507e-09 (1.599616e-09 with ghost)

mpiexec -n 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.035096e-07 (1.035096e-07 with ghost)


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined

grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)


--- TEST predefined and 6th order --

mpiexec -n 2 xterm -e gdb --args tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc


mpirun -np 2 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc



mpirun -np 1 tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined -solver=petsc
 grid=0 (square) max. rel. err=1.123300e-07 (1.123300e-07 with ghost)

============================
==== Aug 18, 2016

--- TEST predefined and 6th order --


tcm3 -g=square32p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)

tcm3 -g=square64p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

tcm3 -g=square128p.order6.hdf -trig -neumann -order=6 -predefined
 grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)




tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
 grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)

============================
==== Aug 17, 2016

TEST sixth-order for Jeff

*FIXED* laplacianFDCoefficients>c (old style)

Looks OK -- ratio 64
tcm3 -g=square64p.order6.hdf -trig -neumann -order=6
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.017155e-07 (1.017155e-07 with ghost)
G64: grid=0 (square) max. rel. err=1.596434e-09 (1.596434e-09 with ghost)
G128:grid=0 (square) max. rel. err=2.472844e-11 (2.472844e-11 with ghost)


ORDER=6  -- but results are only 4th-order
tcm3 -g=square32p.order6.hdf -trig -neumann -order=6
G32: grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)
G64: grid=0 (square) max. rel. err=1.031297e-06 (1.031297e-06 with ghost)
G128:grid=0 (square) max. rel. err=6.449760e-08 (6.449760e-08 with ghost)


tcm3 -g=square8p.order6.hdf -trig -neumann -order=6



ORDER=4
tcm3 -g=square32p.order4.hdf -trig -neumann -order=4
grid=0 (square) max. rel. err=1.645847e-05 (1.645847e-05 with ghost)


-- periodic order 2
tcm3 -g=square32p.order2.hdf -trig -neumann
grid=0 (square) max. rel. err=3.218964e-03 (3.218964e-03 with ghost)


============================
==== July 29, 2016

Test new constraint

tcmConstraint square8.order2.hdf -solver=petsc


tcmConstraint square8.order2.hdf -neumann

****** Oct 11-12, 14 2015.

--- Neumann:

tcmConstraint square8.order2.hdf -neumann

tcmConstraint square8.order2.hdf -neumann -debug=63 >! junk

---- Dirichlet OK: 

tcmConstraint square8.order2.hdf -dirichlet -debug=63 >! junk

tcmConstraint square8.order2.hdf -dirichlet




****** July 1, 2015

mpirun -np 4 tcm3 square32.order2.hdf -solver=petsc -testCommunicator




mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc

PETSC TEST ROUTINE: 
mpiexec -n 2  ex2

 mpiexec -n 2 ex2
Norm of error 0.000591671 iterations 118, m=101, n=101

****** June 30, 2015  --  TEST PETSc communicator

PETSC TEST ROUTINE: 
mpiexec -n 4  ex4

mpiexec -n 2 xterm -e gdb ./tcm3 
run square32.order2.hdf -solver=petsc -testCommunicator


mpirun -np 2 tcm3 square32.order2.hdf -solver=petsc
mpirun -np 1 tcm3 square32.order2.hdf -solver=petsc




*** 2015/05/16 -- more tests on variable coefficients

CIC: 
tcm3 cice2.order2 -mixed

cice4
grid=0 (square) max. rel. err=7.683513e-06 (7.683513e-06 with ghost)
grid=1 (Annulus) max. rel. err=4.251094e-05 (4.251094e-05 with ghost)

cice2:
 grid=0 (square) max. rel. err=4.763309e-05 (4.763309e-05 with ghost)
 grid=1 (Annulus) max. rel. err=2.225289e-04 (2.225289e-04 with ghost)


tcm3 square20pn.order2 -mixed

** square + MIXED BC: exact:
tcm3 square8.order2 -mixed
 grid=0 (square) max. rel. err=6.145234e-16 (9.217852e-16 with ghost)
yale/square8.order2///mixed: error                                : err = 6.15e-16, cpu=1.7e-05(s)  




tcm3 cice2.order2  

*** tcm3 square20.order2


.....solver: size = 2.38e+05 (bytes), grid-pts=625, reals/grid-pt=47.51 
 grid=0 (square) max. rel. err=1.255672e-15 (1.255672e-15 with ghost)
Maximum relative error with dirichlet bc's= 1.255672e-15 (1.255672e-15 with ghost)
yale/square20.order2///dirichlet: error                                                     : err = 1.26e-15, cpu=2.9e-05(s)  
Oges::allocateWorkSpace: numberOfNonzeros=3366 fillinRatio=2.000000e+01
allocateWorkSpace: numberOfEquations=625, nsp = 67320, fillinRatio= 20, numberOfNonzeros = 3366
residual=0.00e+00, time for 1st solve of the Neumann problem = 1.77e-03 (iterations=0)
residual=0.00e+00, time for 2nd solve of the Neumann problem = 3.10e-05 (iterations=0)
 grid=0 (square) max. rel. err=4.101861e-15 (4.185572e-15 with ghost)
Maximum relative error with neumann bc's= 4.101861e-15
yale/square20.order2///neumann: error                                                       : err = 4.10e-15, cpu=3.1e-05(s)  



*** 2014/05/18 -- test variable coefficient BC's

tbcc square16.order2 | grep mixed
square5/square/order=2/std/mixed (var-coeff)          : err = 5.33e-15, cpu=1.5e-04(s)  
cic/square/order=2/std/mixed (var-coeff)              : err = 1.24e-14, cpu=3.8e-04(s)  
cic/Annulus/order=2/std/mixed (var-coeff)             : err = 6.22e-15, cpu=1.8e-04(s)  
sib/box/order=2/std/mixed (var-coeff)                 : err = 1.42e-14, cpu=1.1e-02(s)  
sib/north-pole/order=2/std/mixed (var-coeff)          : err = 4.44e-15, cpu=9.1e-04(s)  
sib/south-pole/order=2/std/mixed (var-coeff)          : err = 4.22e-15, cpu=9.0e-04(s)  

checkop.p tbcc
Running: ./tbcc  > tbcc.out
 compare files tbcc.dp.check.new and tbcc.dp.check 
smartDiff: files tbcc.dp.check.new and tbcc.dp.check agree.
      ...tbcc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================




tbc square16.order2



Regression test for tbc : 
checkop.p tbc 

running tbc...
Running: ./tbc  > tbc.out
 compare files tbc.dp.check.new and tbc.dp.check 
smartDiff: files tbc.dp.check.new and tbc.dp.check agree.
      ...tbc appears to be correct
==================================================
============ Test apparently successful ==========
==================================================